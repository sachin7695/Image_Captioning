# -*- coding: utf-8 -*-
"""Copy of Image_captioning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XKjf1gY8z1vW6a35veGGne3fEYMFGuJ6
"""

from google.colab import drive
drive.mount('/content/drive')

import pickle
import string
import tensorflow
import keras
from pickle import dump
from keras.applications.vgg16 import VGG16
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.models import Model
from os import listdir
from pickle import load
from numpy import argmax
from pickle import load
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import load_model
from keras.callbacks import ModelCheckpoint
from numpy import array
from pickle import load
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Embedding
from keras.layers import Dropout
from keras.layers.merge import add
from keras.callbacks import ModelCheckpoint
import matplotlib.pyplot as plt

with open('/content/set_0.pkl','rb') as f:
  data=pickle.load(f)

names=[]
def load_descriptions(doc):
	mapping = dict()
	# process lines
	for line in doc:
		# split line by white space
		tokens = line.split()
		if len(line) < 2:
			continue
		# take the first token as the image id, the rest as the description
		image_id, image_desc = tokens[0], tokens[1:]
	 
		# remove filename from image id
		image_id = image_id.split('.')[0]
		names.append(image_id)
		
		# convert description tokens back to string
		image_desc = ' '.join(image_desc)
		# create the list if needed
		if image_id not in mapping:
			mapping[image_id] = list()
		# store description
		mapping[image_id].append(image_desc)
	return mapping
 
# parse descriptions
descriptions = load_descriptions(data)

def clean_descri(descriptions):
	# prepare translation table for removing punctuation
	table = str.maketrans('', '', string.punctuation)
	for key, desc_list in descriptions.items():
		for i in range(len(desc_list)):
			desc = desc_list[i]
			# tokenize
			desc = desc.split()
			# convert to lower case
			desc = [word.lower() for word in desc]
			# remove punctuation from each token
			desc = [w.translate(table) for w in desc]
			# remove hanging 's' and 'a'
			desc = [word for word in desc if len(word)>1]
			# remove tokens with numbers in them
			desc = [word for word in desc if word.isalpha()]
			# store as string
			desc_list[i] =  ' '.join(desc)
 
# clean descriptions
clean_descri(descriptions)



# convert the loaded descriptions into a vocabulary of words
def to_vocabulary(descriptions):
	# build a list of all description strings
	all_desc = set()
	for key in descriptions.keys():
		[all_desc.update(d.split()) for d in descriptions[key]]
	return all_desc
 
# summarize vocabulary
vocabulary = to_vocabulary(descriptions)
print('Vocabulary Size: %d' % len(vocabulary))

keys=descriptions.keys()
keys=list(keys)
len(keys)

def save_descriptions(descriptions, filename):
	lines = list()
	for key, desc_list in descriptions.items():
		 
		for desc in desc_list:
			lines.append(key + ' ' + desc)
	data = '\n'.join(lines)
	file = open(filename, 'w')
	file.write(data)
	file.close()
 
# save descriptions
save_descriptions(descriptions, 'descriptions.txt')

#keys=[]

def extract_(directory):
    # load the model
    model = VGG16()
    #remove the output layer so as to extract features
    with tensorflow.device('/GPU:0'):
      model = Model(inputs=model.inputs, outputs=model.layers[-2].output)
  
    # summarize
    print(model.summary())
    # extract features from each photo
    features = dict()
    for name in listdir(directory):
        # load an image from file
        
        filename = directory + '/' + name
        image = load_img(filename, target_size=(224, 224))
        # convert the image pixels to a numpy array
        image = img_to_array(image)
        # reshape data for the model
        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
        # prepare the image for the VGG model
        image = preprocess_input(image)
        # get features
        
        with tensorflow.device('/GPU:0'):
          feature = model.predict(image, verbose=0)
        # get image id
        image_id = name.split('.')[0]
        keys.append(name)
        #print(image_id)
        # store feature
        features[image_id] = feature
        print('>%s' % name)
    return features

# directory = '/content/drive/MyDrive/Flicker8k_Dataset'
# features = extract_(directory)
# #print('Extracted Features: %d' % len(features))
# # save to file
# # dump(features, open('features.pkl', 'wb'))

len(keys)

for i in range(len(keys)):
  temp=(keys[i]).split('.')
  keys[i]=temp[0]

train_keys=keys[:6000]
test_keys=keys[6000:]

len(test_keys)

# load doc into memory
def load_doc(filename):
	# open the file as read only
	file = open(filename, 'r')
	# read all text
	text = file.read()
	# close the file
	file.close()
	return text

# load a pre-defined list of photo identifiers
def load_set(filename):
	doc = load_doc(filename)
	dataset = list()
	# process line by line
	for line in doc.split('\n'):
		# skip empty lines
		if len(line) < 1:
			continue
		# get the image identifier
		identifier = line.split('.')[0]
		dataset.append(identifier)
	return set(dataset)

# load clean descriptions into memory
def load_clean_descri(filename, dataset):
    # load document
    doc = load_doc(filename)
    descriptions = dict()
    for line in doc.split('\n'):
        # split line by white space
        tokens = line.split()
        # split id from description
        image_id, image_desc = tokens[0], tokens[1:]
        # skip images not in the set
        if image_id in dataset:
            # create list
            if image_id not in descriptions:
                descriptions[image_id] = list()
            # wrap description in tokens
            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'
            # store
            descriptions[image_id].append(desc)
    return descriptions

# load photo features
def load_photo_features(filename, dataset):
    # load all features
    all_features = load(open(filename, 'rb'))
    # filter features
    features={}
    for k in dataset:
      if(k in all_features.keys()):
        features[k]=all_features[k]
      
    #features = {k: all_features[k] for k in dataset}
    return features

len(train_keys)



train_descriptions = load_clean_descri('descriptions.txt', train_keys)
print('Descriptions: train=%d' % len(train_descriptions))
# photo features
train_features = load_photo_features('features.pkl', train_keys)
print('Photos: train=%d' % len(train_features))

del_key=[]
for key in train_features.keys():
  if(key not in train_descriptions.keys()):
    del_key.append(key)

for key in del_key:
  train_features.pop(key)

print(len(train_features))
print(len(train_descriptions))

train_descriptions.pop('2258277193_586949ec62') #2258277193_586949ec62
train_keys.remove('2258277193_586949ec62')

test_descriptions = load_clean_descri('descriptions.txt', test_keys)
print('Descriptions: test=%d' % len(test_descriptions))
# photo features
test_features = load_photo_features('features.pkl', test_keys)
print('Photos: test=%d' % len(test_features))

del_key=[]
for key in test_features.keys():
  if(key not in test_descriptions.keys()):
    del_key.append(key)
del_key

# convert a dictionary of clean descriptions to a list of descriptions
def to_lines(descriptions):
	all_desc = list()
	for key in descriptions.keys():
		[all_desc.append(d) for d in descriptions[key]]
	return all_desc
 
# fit a tokenizer given caption descriptions
def create_tokenizer(descriptions):
	lines = to_lines(descriptions)
	tokenizer = Tokenizer()
	tokenizer.fit_on_texts(lines)
	return tokenizer
 
# prepare tokenizer
tokenizer = create_tokenizer(train_descriptions)
vocab_size = len(tokenizer.word_index) + 1
print('Vocabulary Size: %d' % vocab_size)

# create sequences of images, input sequences and output words for an image
def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):
    X1, X2, y = list(), list(), list()
    # walk through each image identifier
    for key, desc_list in descriptions.items():
        # walk through each description for the image
        for desc in desc_list:
            # encode the sequence
            seq = tokenizer.texts_to_sequences([desc])[0]
            # split one sequence into multiple X,y pairs
            for i in range(1,len(seq)):
                # split into input and output pair
                in_seq, out_seq = seq[:i], seq[i]
                # pad input sequence
                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                # encode output sequence
                out_seq = to_categorical([out_seq], num_classes=vocab_size)
                # store
                #print(len(seq),len(photos))
                #X1.append(photos[key][0])
                X2.append(in_seq)
                y.append(out_seq)
    
    return array(X1), array(X2), array(y)

# calculate the length of the description with the most words
def max_length(descrip):
    lines = to_lines(descrip)
    a=max(len(d.split()) for d in lines)
    return a

def define_model(vocab_size, max_length):
    # feature extractor model
    inputs1 = Input(shape=(4096,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)
    # sequence model
    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = LSTM(256)(se2)
    # decoder model
    decoder1 = add([fe2, se3])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)
    # tie it together [image, seq] [word]
    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')
     # summarize model
    print(model.summary())
    plot_model(model, to_file='model.png', show_shapes=True)
    return model

import numpy as np

# create data generator to get data in batch (avoids session crash)
def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):
    # loop over images
    X1, X2, y = list(), list(), list()
    n = 0
    while 1:
        for key in data_keys:
            n += 1
            captions = mapping[key]
            # process each caption
            for caption in captions:
                # encode the sequence
                seq = tokenizer.texts_to_sequences([caption])[0]
                # split the sequence into X, y pairs
                for i in range(1, len(seq)):
                    # split into input and output pairs
                    in_seq, out_seq = seq[:i], seq[i]
                    # pad input sequence
                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                    # encode output sequence
                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
                    
                    # store the sequences
                    X1.append(features[key][0])
                    X2.append(in_seq)
                    y.append(out_seq)
            if n == batch_size:
                X1, X2, y = np.array(X1), np.array(X2), np.array(y)
                yield [X1, X2], y
                X1, X2, y = list(), list(), list()
                n = 0

# create data generator to get data in batch (avoids session crash)
def test_data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):
    # loop over images
    X1, X2, y = list(), list(), list()
    for key in data_keys:
            
            captions = mapping[key]
            # process each caption
            for caption in captions:
                # encode the sequence
                seq = tokenizer.texts_to_sequences([caption])[0]
                # split the sequence into X, y pairs
                for i in range(1, len(seq)):
                    # split into input and output pairs
                    in_seq, out_seq = seq[:i], seq[i]
                    # pad input sequence
                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                    # encode output sequence
                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
                    
                    # store the sequences
                    X1.append(features[key][0])
                    X2.append(in_seq)
                    y.append(out_seq)
            #if n == batch_size:
            X1, X2, y = np.array(X1), np.array(X2), np.array(y)
            yield [X1, X2], y

filename = 'model_1'
checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')

tokenizer = create_tokenizer(train_descriptions)
vocab_size = len(tokenizer.word_index) + 1
test_tokenizer=create_tokenizer(test_descriptions)
test_vocab=len(test_tokenizer.word_index)+1
print('Vocabulary Size: %d' % vocab_size)
# determine the maximum sequence length
max_length = 31
print('Description Length: %d' % max_length)
model = define_model(vocab_size, max_length)
# train the model, run epochs manually and save after each epoch
epochs = 10
steps = len(train_descriptions)
loss=[]
for i in range(epochs):
    # create the data generator
    generator = data_generator(train_keys,train_descriptions, train_features, tokenizer, max_length, vocab_size,32)
    #test_generator=test_data_generator(test_keys[:32],test_descriptions,test_features,test_tokenizer,max_length,test_vocab,32)
    # fit for one epoch
    with tensorflow.device('/GPU:0'):
      history=model.fit(generator, epochs=1, steps_per_epoch=steps/32, verbose=1)
      loss.append(history.history['loss'])      
    # save model
    model.save('model_' + str(i) + '.h5')

plt.plot(loss,[1,2,3,4,5,6,7,8,9,10])
plt.show()

from tqdm.notebook import tqdm

def idx_to_word(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

# generate caption for an image
def predict_caption(model, image, tokenizer, max_length):
    # add start tag for generation process
    in_text = 'startseq'
    # iterate over the max length of sequence
    for i in range(max_length):
        # encode input sequence
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        # pad the sequence
        sequence = pad_sequences([sequence], max_length)
        # predict next word
        y_pred = model.predict([image, sequence], verbose=0)
        # get index with high probability
        y_pred = np.argmax(y_pred)
        
        # convert index to word
        word = idx_to_word(y_pred, tokenizer)
        # stop if word not found
        if word is None:
            break
        # append word as input for generating next word
        in_text += " " + word
        # stop if we reach end tag
        if word == 'endseq':
            break
      
    return in_text

test_descriptions = load_clean_descri('descriptions.txt', test_keys)
print('Descriptions: test=%d' % len(test_descriptions))
# photo features
test_features = load_photo_features('features.pkl', test_keys)
print('Photos: test=%d' % len(test_features))

from nltk.translate.bleu_score import corpus_bleu
# validate with test data
actual, predicted = list(), list()

for key in tqdm(test_keys):
    # get actual caption
    captions = test_descriptions[key]
    # predict the caption for image
    with tensorflow.device('/GPU:0'):
      y_pred = predict_caption(model, test_features[key], tokenizer, max_length) 
    # split into words
    actual_captions = [caption.split() for caption in captions]
    y_pred = y_pred.split()
    # append to the list
    actual.append(actual_captions)
    predicted.append(y_pred)
    
# calcuate BLEU score
print("BLEU-1: %f" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))
print("BLEU-2: %f" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))

from PIL import Image
import matplotlib.pyplot as plt
def  generate_output(image_name):
    # load the image
    # image_name = "1001773457_577c3a7d70.jpg"
    image_id = image_name.split('.')[0]
    img_path = "/content/drive/MyDrive/Flicker8k_Dataset/"+ image_name
    image = Image.open(img_path)
    captions = test_descriptions[image_id]
    print('---------------------Actual---------------------')
    for caption in captions:
        print(caption)
    # predict the caption
    y_pred = predict_caption(model, test_features[image_id], tokenizer, max_length)
    print('--------------------Predicted--------------------')
    print(y_pred)
    plt.imshow(image)

generate_output("3348811097_0e09baa26f.jpg")

generate_output("3131632154_098f86f4cb.jpg")

generate_output("1662261486_db967930de.jpg")

